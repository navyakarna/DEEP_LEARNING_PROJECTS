{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XEi2CE1n0O3"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCAgsM0zn0O4"
      },
      "source": [
        "# Render a colored point cloud\n",
        "\n",
        "This tutorial shows how to:\n",
        "- set up a renderer\n",
        "- render the point cloud\n",
        "- vary the rendering settings such as compositing and camera position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6CMLMg6n0O4"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfvES3AHn0O4"
      },
      "source": [
        "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dzu3Wl5Fn0O4",
        "outputId": "9ef0665b-0ef7-4861-a396-531f31b39a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath) (4.66.6)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.12.2)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=0a2a62e76f1db54c07079a7b24c4184b4999f10ecab5878d78091c7feb392099\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built iopath\n",
            "Installing collected packages: portalocker, iopath\n",
            "Successfully installed iopath-0.1.10 portalocker-3.0.0\n",
            "Trying to install wheel for PyTorch3D\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt251/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt251/pytorch3d-0.7.8-cp310-cp310-linux_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (3.0.0)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.8\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import subprocess\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "    version_str=\"\".join([\n",
        "        f\"py3{sys.version_info.minor}_cu\",\n",
        "        torch.version.cuda.replace(\".\",\"\"),\n",
        "        f\"_pyt{pyt_version_str}\"\n",
        "    ])\n",
        "    !pip install iopath\n",
        "    if sys.platform.startswith(\"linux\"):\n",
        "        print(\"Trying to install wheel for PyTorch3D\")\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "        pip_list = !pip freeze\n",
        "        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for  i in pip_list)\n",
        "    if need_pytorch3d:\n",
        "        print(f\"failed to find/install wheel for {version_str}\")\n",
        "if need_pytorch3d:\n",
        "    print(\"Installing PyTorch3D from source\")\n",
        "    !pip install ninja\n",
        "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0MWEQgk3n0O4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Util function for loading point clouds|\n",
        "import numpy as np\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVOrthographicCameras,\n",
        "    PointsRasterizationSettings,\n",
        "    PointsRenderer,\n",
        "    PulsarPointsRenderer,\n",
        "    PointsRasterizer,\n",
        "    AlphaCompositor,\n",
        "    NormWeightedCompositor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5WsqIDAn0O5"
      },
      "source": [
        "### Load a point cloud and corresponding colors\n",
        "\n",
        "Load and create a **Point Cloud** object.\n",
        "\n",
        "**Pointclouds** is a unique datastructure provided in PyTorch3D for working with batches of point clouds of different sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRF7_v7Qn0O5"
      },
      "source": [
        "If running this notebook using **Google Colab**, run the following cell to fetch the pointcloud data and save it at the path `data/PittsburghBridge`:\n",
        "If running locally, the data is already available at the correct path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nvXTeiHjn0O5",
        "outputId": "4e6b6032-a4a8-41f6-c042-3a98bfb9a828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-17 17:19:57--  https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.33, 13.227.219.10, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5701352 (5.4M) [application/zip]\n",
            "Saving to: ‘data/PittsburghBridge/pointcloud.npz’\n",
            "\n",
            "pointcloud.npz      100%[===================>]   5.44M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-12-17 17:19:57 (73.1 MB/s) - ‘data/PittsburghBridge/pointcloud.npz’ saved [5701352/5701352]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/PittsburghBridge\n",
        "!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KGcqGDOWn0O5"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set paths\n",
        "DATA_DIR = \"./data\"\n",
        "obj_filename = os.path.join(DATA_DIR, \"PittsburghBridge/pointcloud.npz\")\n",
        "\n",
        "# Load point cloud\n",
        "pointcloud = np.load(obj_filename)\n",
        "verts = torch.Tensor(pointcloud['verts']).to(device)\n",
        "\n",
        "rgb = torch.Tensor(pointcloud['rgb']).to(device)\n",
        "\n",
        "point_cloud = Pointclouds(points=[verts], features=[rgb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg4KYSGCn0O5"
      },
      "source": [
        "## Create a renderer\n",
        "\n",
        "A renderer in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here we initialize some of these components and use default values for the rest.\n",
        "\n",
        "In this example we will first create a **renderer** which uses an **orthographic camera**, and applies **alpha compositing**. Then we learn how to vary different components using the modular API.  \n",
        "\n",
        "[1] <a href=\"https://arxiv.org/abs/1912.08804\">SynSin: End to end View Synthesis from a Single Image.</a> Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson. CVPR 2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mgiSbY5Un0O5"
      },
      "outputs": [],
      "source": [
        "# Initialize a camera.\n",
        "R, T = look_at_view_transform(20, 10, 0)\n",
        "cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "# and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
        "raster_settings = PointsRasterizationSettings(\n",
        "    image_size=512,\n",
        "    radius = 0.003,\n",
        "    points_per_pixel = 10\n",
        ")\n",
        "\n",
        "\n",
        "# Create a points renderer by compositing points using an alpha compositor (nearer points\n",
        "# are weighted more heavily). See [1] for an explanation.\n",
        "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
        "renderer = PointsRenderer(\n",
        "    rasterizer=rasterizer,\n",
        "    compositor=AlphaCompositor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg_HQJuyn0O5"
      },
      "outputs": [],
      "source": [
        "images = renderer(point_cloud)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HBjEBM5n0O5"
      },
      "source": [
        "We will now modify the **renderer** to use **alpha compositing** with a set background color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW_tTmKen0O5"
      },
      "outputs": [],
      "source": [
        "renderer = PointsRenderer(\n",
        "    rasterizer=rasterizer,\n",
        "    # Pass in background_color to the alpha compositor, setting the background color\n",
        "    # to the 3 item tuple, representing rgb on a scale of 0 -> 1, in this case blue\n",
        "    compositor=AlphaCompositor(background_color=(0, 0, 1))\n",
        ")\n",
        "images = renderer(point_cloud)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtyaj_mXn0O5"
      },
      "source": [
        "In this example we will first create a **renderer** which uses an **orthographic camera**, and applies **weighted compositing**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPq6ah5In0O5"
      },
      "outputs": [],
      "source": [
        "# Initialize a camera.\n",
        "R, T = look_at_view_transform(20, 10, 0)\n",
        "cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "# and blur_radius=0.0. Refer to rasterize_points.py for explanations of these parameters.\n",
        "raster_settings = PointsRasterizationSettings(\n",
        "    image_size=512,\n",
        "    radius = 0.003,\n",
        "    points_per_pixel = 10\n",
        ")\n",
        "\n",
        "\n",
        "# Create a points renderer by compositing points using an weighted compositor (3D points are\n",
        "# weighted according to their distance to a pixel and accumulated using a weighted sum)\n",
        "renderer = PointsRenderer(\n",
        "    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "    compositor=NormWeightedCompositor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UALGrefHn0O5"
      },
      "outputs": [],
      "source": [
        "images = renderer(point_cloud)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qzFI5hMn0O5"
      },
      "source": [
        "We will now modify the **renderer** to use **weighted compositing** with a set background color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX1VSlfVn0O6"
      },
      "outputs": [],
      "source": [
        "renderer = PointsRenderer(\n",
        "    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "    # Pass in background_color to the norm weighted compositor, setting the background color\n",
        "    # to the 3 item tuple, representing rgb on a scale of 0 -> 1, in this case red\n",
        "    compositor=NormWeightedCompositor(background_color=(1,0,0))\n",
        ")\n",
        "images = renderer(point_cloud)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM-b4Iu0n0O6"
      },
      "source": [
        "## Using the pulsar backend\n",
        "\n",
        "Switching to the pulsar backend is easy! The pulsar backend has a compositor built-in, so the `compositor` argument is not required when creating it (a warning will be displayed if you provide it nevertheless). It pre-allocates memory on the rendering device, that's why it needs the `n_channels` at construction time.\n",
        "\n",
        "All parameters for the renderer forward function are batch-wise except the background color (in this example, `gamma`) and you have to provide as many values as you have examples in your batch. The background color is optional and by default set to all zeros. You can find a detailed explanation of how gamma influences the rendering function here in the paper [Fast Differentiable Raycasting for Neural Rendering using\n",
        "Sphere-based Representations](https://arxiv.org/pdf/2004.07484.pdf).\n",
        "\n",
        "You can also use the `native` backend for the pulsar backend which already provides access to point opacity. The native backend can be imported from `pytorch3d.renderer.points.pulsar`; you can find examples for this in the folder `docs/examples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzs33CNQn0O6"
      },
      "outputs": [],
      "source": [
        "renderer = PulsarPointsRenderer(\n",
        "    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "    n_channels=4\n",
        ").to(device)\n",
        "\n",
        "images = renderer(point_cloud, gamma=(1e-4,),\n",
        "                  bg_col=torch.tensor([0.0, 1.0, 0.0, 1.0], dtype=torch.float32, device=device))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.axis(\"off\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmVlv6-3n0O6"
      },
      "source": [
        "### View pointclouds in Plotly figures\n",
        "\n",
        "Here we use the PyTorch3D function `plot_scene` to render the pointcloud in a Plotly figure. `plot_scene` returns a plotly figure with trace and subplots defined by the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMrG_9ofn0O6"
      },
      "outputs": [],
      "source": [
        "plot_scene({\n",
        "    \"Pointcloud\": {\n",
        "        \"person\": point_cloud\n",
        "    }\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4xjuTinn0O6"
      },
      "source": [
        "We will now render a batch of pointclouds. The first pointcloud is the same as above, and the second is all-black and offset by 2 in all dimensions so we can see them on the same plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNn20slzn0O6"
      },
      "outputs": [],
      "source": [
        "point_cloud_batch = Pointclouds(points=[verts, verts + 2], features=[rgb, torch.zeros_like(rgb)])\n",
        "# render both in the same plot in different traces\n",
        "fig = plot_scene({\n",
        "    \"Pointcloud\": {\n",
        "        \"person\": point_cloud_batch[0],\n",
        "        \"person2\": point_cloud_batch[1]\n",
        "    }\n",
        "})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzMIMIqFn0O6"
      },
      "outputs": [],
      "source": [
        "# render both in the same plot in one trace\n",
        "fig = plot_scene({\n",
        "    \"Pointcloud\": {\n",
        "        \"2 people\": point_cloud_batch\n",
        "    }\n",
        "})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNgw_9VFn0O6"
      },
      "source": [
        "For batches, we can also use `plot_batch_individually` to avoid constructing the scene dictionary ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQQVtp68n0O6"
      },
      "outputs": [],
      "source": [
        "# render both in 1 row in different subplots\n",
        "fig2 = plot_batch_individually(point_cloud_batch, ncols=2)\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uREQznun0O6"
      },
      "outputs": [],
      "source": [
        "# modify the plotly figure height and width\n",
        "fig2.update_layout(height=500, width=500)\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQY1bQWn0O6"
      },
      "source": [
        "We can also modify the axis arguments and axis backgrounds for either function, and title our plots in `plot_batch_individually`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bdE6Zbun0O6"
      },
      "outputs": [],
      "source": [
        "fig3 = plot_batch_individually(\n",
        "    point_cloud_batch,\n",
        "    xaxis={\"backgroundcolor\":\"rgb(200, 200, 230)\"},\n",
        "    yaxis={\"backgroundcolor\":\"rgb(230, 200, 200)\"},\n",
        "    zaxis={\"backgroundcolor\":\"rgb(200, 230, 200)\"},\n",
        "    subplot_titles=[\"Pointcloud1\", \"Pointcloud2\"], # this should have a title for each subplot, titles can be \"\"\n",
        "    axis_args=AxisArgs(showgrid=True))\n",
        "fig3.show()"
      ]
    }
  ],
  "metadata": {
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}